{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naver_news_report_talk_Top2Vec.ipynb",
      "provenance": [],
      "mount_file_id": "1R8Ixn8lXU_NmRS99zjqTEBg3uWi5kAv5",
      "authorship_tag": "ABX9TyMZHobJAwMTvNrWgVPWWhFb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dtype2100/TIL/blob/master/Naver_news_report_talk_Top2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install top2vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1qiLJbPcbMU",
        "outputId": "a87f1edf-344d-4112-e8a5-ecad5833bee1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: top2vec in /usr/local/lib/python3.7/dist-packages (1.0.27)\n",
            "Requirement already satisfied: gensim>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from top2vec) (4.2.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from top2vec) (1.3.5)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.7/dist-packages (from top2vec) (1.5.0)\n",
            "Requirement already satisfied: umap-learn>=0.5.1 in /usr/local/lib/python3.7/dist-packages (from top2vec) (0.5.3)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from top2vec) (1.21.6)\n",
            "Requirement already satisfied: hdbscan>=0.8.27 in /usr/local/lib/python3.7/dist-packages (from top2vec) (0.8.28)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=4.0.0->top2vec) (6.0.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=4.0.0->top2vec) (1.4.1)\n",
            "Requirement already satisfied: cython>=0.27 in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.27->top2vec) (0.29.28)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.27->top2vec) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.27->top2vec) (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->hdbscan>=0.8.27->top2vec) (3.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from umap-learn>=0.5.1->top2vec) (4.64.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.7/dist-packages (from umap-learn>=0.5.1->top2vec) (0.5.7)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn>=0.5.1->top2vec) (0.51.2)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn>=0.5.1->top2vec) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn>=0.5.1->top2vec) (57.4.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->top2vec) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->top2vec) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->top2vec) (1.15.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from wordcloud->top2vec) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install top2vec[sentence_encoders]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FDO7GzT-u-j",
        "outputId": "76967338-0a77-4898-f637-8ffc8b626616"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: top2vec[sentence_encoders] in /usr/local/lib/python3.7/dist-packages (1.0.27)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_encoders]) (1.5.0)\n",
            "Requirement already satisfied: gensim>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_encoders]) (4.2.0)\n",
            "Requirement already satisfied: hdbscan>=0.8.27 in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_encoders]) (0.8.28)\n",
            "Requirement already satisfied: umap-learn>=0.5.1 in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_encoders]) (0.5.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_encoders]) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_encoders]) (1.21.6)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_encoders]) (2.8.2)\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_encoders]) (0.12.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from top2vec[sentence_encoders]) (2.8.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=4.0.0->top2vec[sentence_encoders]) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=4.0.0->top2vec[sentence_encoders]) (6.0.0)\n",
            "Requirement already satisfied: cython>=0.27 in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.27->top2vec[sentence_encoders]) (0.29.28)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.27->top2vec[sentence_encoders]) (1.0.2)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.7/dist-packages (from hdbscan>=0.8.27->top2vec[sentence_encoders]) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->hdbscan>=0.8.27->top2vec[sentence_encoders]) (3.1.0)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn>=0.5.1->top2vec[sentence_encoders]) (0.51.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from umap-learn>=0.5.1->top2vec[sentence_encoders]) (4.64.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.7/dist-packages (from umap-learn>=0.5.1->top2vec[sentence_encoders]) (0.5.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn>=0.5.1->top2vec[sentence_encoders]) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn>=0.5.1->top2vec[sentence_encoders]) (0.34.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->top2vec[sentence_encoders]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->top2vec[sentence_encoders]) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->top2vec[sentence_encoders]) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->top2vec[sentence_encoders]) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->top2vec[sentence_encoders]) (1.0.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->top2vec[sentence_encoders]) (0.25.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->top2vec[sentence_encoders]) (2.8.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->top2vec[sentence_encoders]) (3.1.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->top2vec[sentence_encoders]) (1.1.2)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->top2vec[sentence_encoders]) (0.5.3)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->top2vec[sentence_encoders]) (2.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->top2vec[sentence_encoders]) (4.2.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->top2vec[sentence_encoders]) (14.0.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->top2vec[sentence_encoders]) (1.1.0)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow->top2vec[sentence_encoders]) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->top2vec[sentence_encoders]) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->top2vec[sentence_encoders]) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->top2vec[sentence_encoders]) (1.14.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->top2vec[sentence_encoders]) (2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->top2vec[sentence_encoders]) (1.44.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->top2vec[sentence_encoders]) (1.6.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->top2vec[sentence_encoders]) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->top2vec[sentence_encoders]) (1.5.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->top2vec[sentence_encoders]) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->top2vec[sentence_encoders]) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->top2vec[sentence_encoders]) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->top2vec[sentence_encoders]) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->top2vec[sentence_encoders]) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->top2vec[sentence_encoders]) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->top2vec[sentence_encoders]) (1.8.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->top2vec[sentence_encoders]) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->top2vec[sentence_encoders]) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->top2vec[sentence_encoders]) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->top2vec[sentence_encoders]) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->top2vec[sentence_encoders]) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->top2vec[sentence_encoders]) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->top2vec[sentence_encoders]) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->top2vec[sentence_encoders]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->top2vec[sentence_encoders]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->top2vec[sentence_encoders]) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->top2vec[sentence_encoders]) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->top2vec[sentence_encoders]) (3.2.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from wordcloud->top2vec[sentence_encoders]) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BElbt0oMgsYR",
        "outputId": "34efd569-d368-4d53-d4b3-6e8d69c94374"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Using cached konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.6)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Using cached JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (4.2.0)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.3.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pdfminer.six"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDw5nz07namp",
        "outputId": "a49c655b-b86c-4c81-f4a7-60298ae8da1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20220506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from pdfminer.six) (2.0.12)\n",
            "Collecting cryptography~=36.0.0\n",
            "  Downloading cryptography-36.0.2-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 43.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography~=36.0.0->pdfminer.six) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography~=36.0.0->pdfminer.six) (2.21)\n",
            "Installing collected packages: cryptography, pdfminer.six\n",
            "Successfully installed cryptography-36.0.2 pdfminer.six-20220506\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pej7JrU4XFmY"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "import pandas as pd\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "\n",
        "from pdfminer.high_level import extract_pages\n",
        "from pdfminer.high_level import extract_text\n",
        "from top2vec import Top2Vec\n",
        "from konlpy.tag import Okt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5GffadL5ZdRt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34f5e6ad-f4d0-4347-9633-827481723252"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/openpyxl/styles/stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
            "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_text = repr(file_path)\n",
        "df5 = file_text.replace('\\\\n', '')"
      ],
      "metadata": {
        "id": "QHNLzMxyn4BK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "nMTop3ct2fWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df6.rename(columns={'title':'제목'}, inplace=True)\n",
        "df7.rename(columns={'title':'제목'}, inplace=True)"
      ],
      "metadata": {
        "id": "KyUi4uqs2fTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([df1, df2, df3, df4, df6, df7]) # 파일 합치기 (종합할 데이터 없으면 각주처리)"
      ],
      "metadata": {
        "id": "u9AbqPiPtZhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df = pd.concat([df1, df2, df3, df4]) # 파일 합치기 (종합할 데이터 없으면 각주처리)\n",
        "title_df = df[df['제목'].str.contains('네이버')] # 해당 키워드를 포함한 제목 추출 (불러온 데이터에 따라 키워드 변경)\n",
        "title_df.drop_duplicates(subset=['제목'], inplace=True) # 중복 제거\n",
        "new_df = pd.DataFrame({'일자' : pd.to_datetime(title_df['일자'], format='%Y%m%d'), '제목' : title_df['제목'], 'URL' : title_df['URL']}) # datetime 데이터프레임 생성\n",
        "mask = (new_df['일자'] >= '2020-03-31') & (new_df['일자'] <= '2020-08-31') # 날짜 변경\n",
        "period_df = new_df.loc[mask] # 날짜 인덱싱\n",
        "\n",
        "str_df = period_df['제목'] + df5\n",
        "\n",
        "lis_df = list(str_df) # 데이터 타입 list로 변경"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WsdYyc7hMrS",
        "outputId": "51926faf-4c34-46de-ce5c-65545a709f1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py:311: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  return func(*args, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "str_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvjR-YtNxLvP",
        "outputId": "3e406f0f-ffb6-4264-892f-494c95e5d6d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1080     [네이버 테크인사이드] ㊾ 농업도 '라인웍스'로 디지털전환... \"이제 카톡으로 업...\n",
              "1081     데스커, 네이버 브랜드데이 ‘게이밍 데스크’ 첫선\"2020. 7. 8 인터넷/게임 ...\n",
              "1082     데스커, '네이버 브랜드데이' 진행 게이밍 데스크 최초 공개\"2020. 7. 8 인...\n",
              "1083     블록체인 사업 채비 마친 '네이버 라인' 다음 행보에 쏠린 눈\"2020. 7. 8 ...\n",
              "1084     '온라인 장보기' 내놓은 네이버, 쿠팡 마켓컬리 넘어설까\"2020. 7. 8 인터넷...\n",
              "                               ...                        \n",
              "12866    네이버웹툰, 유료콘텐츠 하루 거래액 30억 돌파 업계 최초\"2020. 7. 8 인터...\n",
              "13697    \"다음주까지 못 기다려\" 네이버웹툰, 1일 거래액 30억 돌파\"2020. 7. 8 ...\n",
              "15726    네이버웹툰 하루 거래액 30억원 돌파 올해 8000억원 목표\"2020. 7. 8 인...\n",
              "15797    ‘승승장구’ 네이버웹툰, 글로벌 하루 거래액 30억 돌파 연간 1조원 가능할까\"20...\n",
              "17740    페이스북 신무기는 ‘리브라 페이’? 네이버 카카오와 경쟁 예고\"2020. 7. 8 ...\n",
              "Name: 제목, Length: 1013, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7owoUZ38xLsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.concat([df1, df2, df3, df4]) # 파일 합치기 (종합할 데이터 없으면 각주처리)\n",
        "# title_df = df[df['제목'].str.contains('네이버')] # 해당 키워드를 포함한 제목 추출 (불러온 데이터에 따라 키워드 변경)\n",
        "# title_df.drop_duplicates(subset=['제목'], inplace=True) # 중복 제거\n",
        "# new_df = pd.DataFrame({'일자' : pd.to_datetime(title_df['일자'], format='%Y%m%d'), '제목' : title_df['제목'], 'URL' : title_df['URL']}) # datetime 데이터프레임 생성\n",
        "# mask = (new_df['일자'] >= '2020-03-31') & (new_df['일자'] <= '2020-08-31') # 날짜 변경\n",
        "# period_df = new_df.loc[mask] # 날짜 인덱싱\n",
        "\n",
        "# okt = Okt()\n",
        "# stop_words = '네이버와 카카오'\n",
        "# stop_words = set(stop_words.split(' '))\n",
        "# word_tokens = okt.morphs(str(period_df['제목']))\n",
        "# li_df = [word for word in word_tokens if not word in stop_words]\n",
        "\n",
        "# lis_df = list(li_df)"
      ],
      "metadata": {
        "id": "pz8VjOkLaOr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 훈련"
      ],
      "metadata": {
        "id": "mmeBLjkgdMGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed = hub.load(\"https://tfhub.dev/google/nnlm-ko-dim50/2\")\n",
        "model = Top2Vec(documents=lis_df, speed=\"learn\", workers=8, embedding_model=embed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zvKAdILcmNL",
        "outputId": "3813398f-e0b8-4792-f504-d3edbb21bd81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-16 17:24:07,705 - top2vec - INFO - Pre-processing documents for training\n",
            "INFO:top2vec:Pre-processing documents for training\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "2022-05-16 17:30:46,221 - top2vec - INFO - Creating joint document/word embedding\n",
            "INFO:top2vec:Creating joint document/word embedding\n",
            "2022-05-16 17:30:57,526 - top2vec - INFO - Creating lower dimension embedding of documents\n",
            "INFO:top2vec:Creating lower dimension embedding of documents\n",
            "/usr/local/lib/python3.7/dist-packages/numba/np/ufunc/parallel.py:363: NumbaWarning: The TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 9107. The TBB threading layer is disabled.\n",
            "  warnings.warn(problem)\n",
            "2022-05-16 17:31:11,270 - top2vec - INFO - Finding dense areas of documents\n",
            "INFO:top2vec:Finding dense areas of documents\n",
            "2022-05-16 17:31:11,311 - top2vec - INFO - Finding topics\n",
            "INFO:top2vec:Finding topics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Number of Topics"
      ],
      "metadata": {
        "id": "cULYBKocdUFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_num_topics()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KL6B8qbxdEGb",
        "outputId": "f5044334-3546-4037-caef-dc30ef4d2ed8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Topics Sizes"
      ],
      "metadata": {
        "id": "VsEq4B06div6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_sizes, topic_nums = model.get_topic_sizes()"
      ],
      "metadata": {
        "id": "S9zTRaiUdeUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Topics"
      ],
      "metadata": {
        "id": "8NmaUbOpdv4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_words, word_scores, topic_scores, topic_nums = model.search_topics(keywords=[\"네이버\"], num_topics=1)"
      ],
      "metadata": {
        "id": "7gROV1ced0ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pRfNIblef9j",
        "outputId": "4d0932f8-a260-4320-d616-018bd05830a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array(['수익', '실적', '매출', '이익률', '매출액', '트래픽', '수익성', '손익', '이익', 'cash',\n",
              "        '순이익', '마진', '수익률', '밸류에이션', '매출원가', '안정성', '컨센서스', '가격', '고정비',\n",
              "        '컨텐츠', '마케팅비', '추정치', '데이터', '판매량', '사용량', '성장성', 'valuation',\n",
              "        '포트폴리오', '괴리율', '어플리케이션', '순매출', '지배력', '자산', '신사업', '증감', '컨버젼',\n",
              "        '거래', '판매', '거래액', '경쟁력', '외형', '콘텐츠', '수요', '모멘텀', '투자', '역량',\n",
              "        '가치', '투자액', '조달', '지분율'], dtype='<U9')]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Word Clouds"
      ],
      "metadata": {
        "id": "GLLs6nSFfcqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# topic_words, word_scores, topic_scores, topic_nums = model.search_topics(keywords=[\"네이버\"], num_topics=1)\n",
        "# for topic in topic_nums:\n",
        "#     model.generate_topic_wordcloud(topic)"
      ],
      "metadata": {
        "id": "XX8tkUDrej4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Search Documents by Topic"
      ],
      "metadata": {
        "id": "T07x8tNKf7rs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents, document_scores, document_ids = model.search_documents_by_topic(topic_num=1, num_docs=10)\n",
        "for doc, score, doc_id in zip(documents, document_scores, document_ids):\n",
        "    print(f\"Document: {doc_id}, Score: {score}\")\n",
        "    print(\"-----------\")\n",
        "    print(doc)\n",
        "    print(\"-----------\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "M55KeDH6fYlx",
        "outputId": "79213d4d-532a-40bd-d412-8d56cd6b9b14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-bfe6eaf9a892>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_documents_by_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_docs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Document: {doc_id}, Score: {score}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-----------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/top2vec/Top2Vec.py\u001b[0m in \u001b[0;36msearch_documents_by_topic\u001b[0;34m(self, topic_num, num_docs, return_documents, reduced)\u001b[0m\n\u001b[1;32m   2310\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2312\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_topic_num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2313\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_topic_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/top2vec/Top2Vec.py\u001b[0m in \u001b[0;36m_validate_topic_num\u001b[0;34m(self, topic_num, reduced)\u001b[0m\n\u001b[1;32m   1173\u001b[0m             \u001b[0mtopic_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopic_vectors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtopic_num\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtopic_count\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Invalid topic number: valid original topics numbers are 0 to {topic_count}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_topic_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Invalid topic number: valid original topics numbers are 0 to 0."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Semantic Search Documents by Keywords"
      ],
      "metadata": {
        "id": "odSuWj9OhT0J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents, document_scores, document_ids = model.search_documents_by_keywords(keywords=[\"네이버\"], num_docs=10)\n",
        "for doc, score, doc_id in zip(documents, document_scores, document_ids):\n",
        "    print(f\"Document: {doc_id}, Score: {score}\")\n",
        "    print(\"-----------\")\n",
        "    print(doc)\n",
        "    print(\"-----------\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "i33G13bOgsfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Similar Keywords"
      ],
      "metadata": {
        "id": "BnXsJQnHhXJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words, word_scores = model.similar_words(keywords=[\"네이버\"], keywords_neg=[], num_words=20)\n",
        "for word, score in zip(words, word_scores):\n",
        "    print(f\"{word} {score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQGqEa-4hVP5",
        "outputId": "4eb9ecd6-3f3e-4d13-ee81-caa96f6546d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "포털 0.8457718528943259\n",
            "유튜브 0.7709171159829908\n",
            "사이트 0.7295019292113227\n",
            "야후 0.7166914242224498\n",
            "구글 0.6975502301906484\n",
            "쿠팡 0.6888635452903606\n",
            "인터넷 0.6856421922500521\n",
            "검색 0.6795214642597057\n",
            "옥션 0.669601001074344\n",
            "온라인 0.6578929675391725\n",
            "메신저 0.6222371298666968\n",
            "안드로이드 0.6094683291016594\n",
            "웹툰 0.606801447646618\n",
            "게임빌 0.6039145517125131\n",
            "컴투스 0.6005436266182747\n",
            "넷마블 0.5943452839471339\n",
            "모바일 0.5888586798143296\n",
            "방문자 0.5886071534443067\n",
            "넥슨 0.5852483392140049\n",
            "다운로드 0.5784230559362901\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JPfTv8Q8hYpx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "613817dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "mecab = Mecab()\n",
    "\n",
    "\n",
    "def get_news(url):\n",
    "    headers = {\"user-agent\":\"Mozilla/5.0\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    html = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    article = html.select_one(\"#articleBodyContents\").text\n",
    "    return article\n",
    "\n",
    "url = \"https://news.naver.com/main/read.nhn?mode=LSD&mid=sec&sid1=105&oid=018&aid=0004430108\"\n",
    "\n",
    "news_article = get_news(url)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98a10d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b74541ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(text):\n",
    "    #return sent_tokenize(text)\n",
    "    return split_sentences(text)\n",
    "\n",
    "def get_words(text, isNoun = False):\n",
    "    if isNoun:\n",
    "        return [token[1] for token in mecab.pos(text) if token[1][0] == 'N' and len(token[0]) > 0]\n",
    "    else:\n",
    "        return [token[1] for token in mecab.pos(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95320adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(word_list, min_ratio=0.001, max_ratio=0.5):\n",
    "    keywords = set()\n",
    "    \n",
    "    count_dict = {}\n",
    "    \n",
    "    for word in word_list:\n",
    "        if word in count_dict.keys():\n",
    "            count_dict[word] = count_dict[word] + 1\n",
    "        else:\n",
    "            count_dict[word] = 1\n",
    "            \n",
    "    \n",
    "    for words, cnt in count_dict.items():\n",
    "        word_percentage = cnt / len(word_list)\n",
    "        \n",
    "        if word_percentage >= min_ratio and word_percentage <= max_ratio:\n",
    "            keywords.add(words)\n",
    "            \n",
    "    return keywords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3b12f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'사과', '포도'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_keywords(['바나나', '사과', '바나나', '바나나', '포도'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "753df772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    " for i in range(5 - 1, -0, -1): #순서를 알아보기 위해 작성\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9909eeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_weight (token_list, keywords):\n",
    "    window_start = 0; window_end = -1\n",
    "    \n",
    "    for i in range(len(token_list)): #시작\n",
    "        if token_list[i] in keywords:\n",
    "            window_start = i\n",
    "            break\n",
    "            \n",
    "    for i in range(len(token_list) - 1, -1, -1): #끝\n",
    "        if token_list[i] in keywords:\n",
    "            window_end = i\n",
    "            break\n",
    "            \n",
    "    if window_start > window_end:\n",
    "        return 0\n",
    "    \n",
    "    window_size = window_end - window_start + 1 #질문\n",
    "    \n",
    "    #keyword 갯수\n",
    "    for url in urls:\n",
    "        keyword_cnt = 0\n",
    "    for w in token_list:\n",
    "        if w in keywords:\n",
    "            keyword_cnt += 1\n",
    "    \n",
    "    return keyword_cnt * keyword_cnt / window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e56494d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'urls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24564\\1801604802.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_sentence_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'바나나'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'사과'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'바나나'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'바나나'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'포도'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'사과'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'포도'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24564\\3669296587.py\u001b[0m in \u001b[0;36mget_sentence_weight\u001b[1;34m(token_list, keywords)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m#keyword 갯수\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0murls\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mkeyword_cnt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtoken_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'urls' is not defined"
     ]
    }
   ],
   "source": [
    "get_sentence_weight(['바나나', '사과', '바나나', '바나나', '포도'], {'사과', '포도'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e1556c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(context, no_sentences=5):\n",
    "    word_list = get_words(context, isNoun=False)\n",
    "    keywords = get_keywords(word_list)\n",
    "    \n",
    "    sentence_list = get_sentences(context)\n",
    "    \n",
    "    sentence_weight = []\n",
    "    for sentence in sentence_list:\n",
    "        token_list = get_words(sentence, isNoun=False)\n",
    "        sentence_weight.append((get_sentence_weight(token_list, keywords), sentence))\n",
    "        \n",
    "    sentence_weight.sort(reverse=True)\n",
    "    \n",
    "    return sentence_weight[:no_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1cd44666",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'split_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24564\\251536413.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msumm_sents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msummarize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnews_article\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msumm_sents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24564\\1958460367.py\u001b[0m in \u001b[0;36msummarize\u001b[1;34m(context, no_sentences)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mkeywords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_keywords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0msentence_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_sentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0msentence_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24564\\652778003.py\u001b[0m in \u001b[0;36mget_sentences\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_sentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m#return sent_tokenize(text)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msplit_sentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0misNoun\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'split_sentences' is not defined"
     ]
    }
   ],
   "source": [
    "summ_sents = summarize(news_article, 3)\n",
    "for s in summ_sents:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8706242b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'split_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24564\\1228044385.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msentence_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_sentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnews_article\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msentence_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24564\\652778003.py\u001b[0m in \u001b[0;36mget_sentences\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_sentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m#return sent_tokenize(text)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msplit_sentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0misNoun\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'split_sentences' is not defined"
     ]
    }
   ],
   "source": [
    "sentence_list = get_sentences(news_article)\n",
    "print(len(sentence_list))\n",
    "sentence_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0418e339",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'split_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24564\\1243373708.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msummarize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnews_article\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24564\\1958460367.py\u001b[0m in \u001b[0;36msummarize\u001b[1;34m(context, no_sentences)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mkeywords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_keywords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0msentence_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_sentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0msentence_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_24564\\652778003.py\u001b[0m in \u001b[0;36mget_sentences\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_sentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m#return sent_tokenize(text)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msplit_sentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0misNoun\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'split_sentences' is not defined"
     ]
    }
   ],
   "source": [
    "summarize(news_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8812fdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kss in c:\\users\\user\\anaconda3\\envs\\nlp\\lib\\site-packages (3.3.1.1)\n",
      "Requirement already satisfied: emoji in c:\\users\\user\\anaconda3\\envs\\nlp\\lib\\site-packages (from kss) (1.6.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cdd3a8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Korean Sentence Splitter]: Initializing Pynori...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "['과기정통부, 22일 유영민 장관 등 참석해 기념행사2021년까지 1516억원 투입, 5100여종 데이터 구축민간 클라우드 통한 외부연계체계도..\"개방성 강화\"[이데일리 이재운 기자] 국가 차원의 빅데이터 활용 시대가 열린다.', '새로운 산업 창출과 기존 산업의 변화에 이르는 ‘혁신성장’을 위한 센터가 문을 연다. 10개 분야에 걸쳐 ‘데이터 경제’의 발전을 위한 정부의 청사진을 현실로 구현하는데 앞장선다는 계획이다.', '22일 과학기술정보통신부는 서울 중구 대한상공회의소에서 데이터 생태계 조성과 혁신 성장의 기반 마련을 위한 ‘빅데이터 플랫폼 및 센터’ 출범식 행사를 개최했다.', '유영민 과기정통부 장관을 비롯해 노웅래 국회 과학기술정보방송통신위원회 위원장 등 300여명이 참가했다.', '◇10개 분야 100개 센터..3년간 1516억원 투입이미지: 픽사베이빅데이터는 데이터 활용을 통해 혁신성장을 이루자는 문재인 정부의 경제 성장 핵심 요소중 하나다.', '문재인 대통령이 직접 올 들어 데이터 활용과 이에 따른 정보보호(보안)에 대한 중요성을 강조하기도 했다.', '이런 맥락 속에서 빅데이터센터는 공공과 민간이 협업해 활용도 높은 양질의 데이터를 생산·구축하고, 플랫폼은 이를 수집·분석·유통하는 역할을 담당한다.', '과기정통부는 분야별 플랫폼 10개소와 이와 연계된 기관별 센터 100개소를 구축하는데 3년간 총 1516억원을 투입할 계획이며, 올해 우선 640억원 규모의 사업을 추진하고 있다.', '대상 분야는 △금융(BC카드) △환경(한국수자원공사) △문화(한국문화정보원) △교통(한국교통연구원) △헬스케어(국립암센터) △유통·소비(매일방송) △통신(KT) △중소기업(더존비즈온) △지역경제(경기도청) △산림(한국임업진흥원) 등으로 현재 1차 공모를 통해 72개 빅데이터 센터를 선정했고, 다음달 8일까지 2차 공모를 통해 28개를 추가 선정해 총 100개를 지원, 운영할 계획이다.', '이를 통해 데이터 생태계를 혁신하고 기업의 경쟁력을 제고하는 역할을 수행한다.', '주요 활용 전략·사례를 보면 빅데이터 활용을 통해 ‘신(新) 시장’을 창출하는 방안을 담고 있다.', '금융 플랫폼의 경우 소상공인 신용평가 고도화 등을 통해 금융 취약 계층 대상 중금리 대출이자를 2%p 절감해 연간 1조원의 신규대출을 창출할 전망이다.', '유통·소비와 중소기업 플랫폼은 소상공인이나 중소기업의 폐업률 감소를, 문화 플랫폼은 문화·예술 관람률과 생활체육 참여율을 높이는 방안을 모색한다.', '의료비 절감(헬스케어)과 기업의 매출 향상을 통한 산업 육성(통신·산림) 등도 눈길을 끈다.', '과기정통부 제공◇2021년까지 5100여종 데이터 구축..AI 알고리즘 제공도센터는 우선 분야별 데이터 부족 문제를 해소하기 위해 올해 말까지 시장 수요가 높은 1400여종 신규 데이터를 생산ㆍ구축하고, 사업이 완료되는 2021년까지 총 5100여종 양질의 풍부한 데이터를 생산·구축해 시장에 공급할 계획이다.', '특히 공공과 민간 사이 데이터 파일형식 등이 달라 호환이 제대로 이뤄지지 못한 문제를 해소하기 위해 개방형 표준을 적용하고, 품질관리기준도 마련해 운영한다.', '기업들이 실제 활용 가능한 최신 데이터를 확보하는데도 수개월이 소요된다는 문제점을 개선하기 위한 방안도 추진한다.', '센터와 플랫폼 간 연계체계에는 민간 클라우드를 기반으로 활용하고, 센터에 축적된 데이터도 계속 외부와 개방·공유하며 최신·연속성을 확보한다는 계획이다.', '100개 센터에서 수집된 데이터를 융합·분석한 뒤 맞춤형 데이터 제작 등 양질의 데이터로 재생산하고, 기업들이 필요로 하는 데이터를 원하는 형태로 즉시 활용할 수 있도록 제공할 계획이다.', '다양한 분석 도구는 물론 인공지능(AI) 학습 알고리즘도 제공해 이용자가 보다 사용하기 편리한 환경을 제공한다.', '이밖에 필요한 데이터를 쉽게 등록하고 검색할 수 있도록 기준을 마련하고, 데이터 보유와 관리에 대한 체계(거버넌스)를 논의하는 ‘데이터 얼라이언스’를 구성해 보다 안전하게 이용하는 방안도 마련했다.', '유영민 과기정통부 장관은 “오늘 출범식은 대한민국이 데이터 강국으로 가기 위한 초석을 놓은 자리”라며 “세계 주요국들보다 데이터 경제로 나아가는 발걸음이 다소 늦었지만, 빅데이터 플랫폼과 센터를 지렛대로 우리나라의 낙후된 데이터 생태계를 혁신하고 기업의 경쟁력을 한 단계 제고할 수 있도록 정책적 역량을 집중하겠다”고 밝혔다.', '이재운 (jwlee@edaily.co.kr)네이버 홈에서 ‘이데일리’ 뉴스 [구독하기▶]꿀잼가득 [영상보기▶] , 청춘뉘우스~ [스냅타임▶]＜ⓒ종합 경제정보 미디어 이데일리 - 무단전재 & 재배포 금지＞']\n"
     ]
    }
   ],
   "source": [
    "from kss import split_sentences\n",
    "sentence_list = split_sentences(news_article)\n",
    "print(len(sentence_list))\n",
    "print(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26e30f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"딸기 바나나 사과 파인애플 수박. 바나나 사과 딸기 포도. 복숭아 수박. 파인애플 사과 딸기 바나나\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0057d689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sentence_similarity(sent1, sent2):\n",
    "    node_pos = [\"NNG\", \"NNP\"]\n",
    "    \n",
    "    sent1_list = [token[0] for token in mecab.pos(sent1) if token[1][0] in ['N', 'V']]\n",
    "    sent2_list = [token[0] for token in mecab.pos(sent2) if token[1][0] in ['N', 'V']]\n",
    "    \n",
    "    union = set(sent1_list).union(set(sent2_list))\n",
    "    intersection = set(sent1_list).intersection(set(sent2_list))\n",
    "    \n",
    "    return len(intersection)/len(union)\n",
    "    \n",
    "sentence_similarity(\"나는 치킨을 좋아해\", \"나는 치킨을 싫어해\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2837158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.5        0.16666667 0.8       ]\n",
      " [0.5        0.         0.         0.6       ]\n",
      " [0.16666667 0.         0.         0.        ]\n",
      " [0.8        0.6        0.         0.        ]]\n",
      "[[0.         0.3409091  0.11363637 0.54545456]\n",
      " [0.45454544 0.         0.         0.54545456]\n",
      " [1.         0.         0.         0.        ]\n",
      " [0.57142854 0.4285714  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def buildMatrix(sentences):\n",
    "    sentences_len = len(sentences)\n",
    "    \n",
    "    score = np.ones(sentences_len, dtype=np.float32)\n",
    "    \n",
    "    weighted_edge = np.zeros((sentences_len, sentences_len), dtype=np.float32)\n",
    "    \n",
    "    for i in range (sentences_len):\n",
    "        for j in range(sentences_len):\n",
    "            if i == j:\n",
    "                continue\n",
    "            weighted_edge[i][j] = sentence_similarity(sentences[i], sentences[j])\n",
    "    \n",
    "    print(weighted_edge)\n",
    "    for i in range(sentences_len):\n",
    "        score[i] = weighted_edge[i].sum()\n",
    "        weighted_edge[i] /= score[i]\n",
    "    \n",
    "    print(weighted_edge)\n",
    "    \n",
    "    \n",
    "buildMatrix(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "886f6b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(weighted_edge, eps=0.0001, d=0.85, max_iter = 50):\n",
    "    for iter in range(max_iter):\n",
    "        new_score = (1 - d) * weighted_edge.T.dot(score)\n",
    "        \n",
    "        for diff in abs(new_score - score):\n",
    "            if diff < eps:\n",
    "                return new_score_score\n",
    "        \n",
    "        score = new_score\n",
    "        \n",
    "    return new_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e8fc14-67a4-470e-93d5-fab5d1ec6cce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d8155c-4749-4346-a7c7-4b60cba20326",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
